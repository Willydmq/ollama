# Ollama

Ver en "localhost:3000"

 La version actual de chatbot-ollama no descarga el modelo automaticamente.
 
 Vas a tener que correr este comando cada vez que necesites un modelo nuevo:
 
 docker-compose exec ollama ollama pull llama2  


 Ver v√≠deo:

 [![Alt text](https://img.youtube.com/vi/DEcP4bkvHG4/0.jpg)](https://www.youtube.com/watch?v=DEcP4bkvHG4)

